{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "231de592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mpld3\n",
    "!pip install bs4\n",
    "!pip install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4416028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs # for encoding and decoding\n",
    "from sklearn import feature_extraction\n",
    "import mpld3  #The mpld3 project brings together Matplotlib, the popular Python-based graphing library, and D3js, the popular JavaScript library for creating interactive data visualizations for the web. The result is a simple API for exporting your matplotlib graphics to HTML code which can be used within the browser, within standard web pages, blogs, or tools such as the IPython notebook.\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Clustring Algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation #LDA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD #LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90207ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6746d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path = \"articles1.csv\", selected_column = 'content'):\n",
    "    #Read Data set\n",
    "    Data = pd.read_csv(path, encoding='latin-1')\n",
    "    \n",
    "    #Selecting required columns and rows\n",
    "    Data = Data[[selected_column]]\n",
    "    \n",
    "    #Cleaning Data\n",
    "    Data = Data[pd.notnull(Data[selected_column])]\n",
    "    Data[selected_column] = Data[selected_column].str.replace('â\\x80\\x99',\"'\")\n",
    "    Data[selected_column] = Data[selected_column].str.replace('â\\x80\\x98',\"'\")\n",
    "    Data[selected_column] = Data[selected_column].str.replace('â\\x80\\x9c','\"')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('â\\x80\\x9d','\"')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('â\\x80\\x94','-')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('â\\x80¦','...')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('â\\x80¢','•')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('Ã©','é')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('Ã³','ó')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('Ã¼','ü')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('Ã¡','á')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('_____','')\n",
    "    Data[selected_column] = Data[selected_column].str.replace(\"' '\",'')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('  ',' ')\n",
    "    Data[selected_column] = Data[selected_column].str.replace('   ',' ')\n",
    "    \n",
    "    # Convert dataframe to list\n",
    "    data = Data[selected_column].tolist()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d7e8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from files and clean it. \n",
    "Data = read_data(\"articles1.csv\", 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d930de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d32e4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for sentence tokenizer, to remove numeric tokens and raw #punctuation\n",
    "\n",
    "def sentence_seperator(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def tokenize_only_single(text):\n",
    "    tokens = [word for word in nltk.word_tokenize(text)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]', token) and token.casefold() not in stop_words and not re.search('\\W', token)):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]', token) and token.casefold() not in stop_words and not re.search('\\W', token)):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]', token) and token.casefold() not in stop_words and not re.search('\\W', token)):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens if t]\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_and_stem_single(text):\n",
    "    tokens = [word for word in nltk.word_tokenize(text)]    \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]', token) and token.casefold() not in stop_words and not re.search('\\W', token)):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens if t]\n",
    "    return stems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9da400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Topics will be list of sentences\n",
    "def tf_idf(topics = []):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=0.95, \n",
    "        #max_features=200000, \n",
    "        min_df=0.05, \n",
    "        stop_words='english', \n",
    "        #use_idf=True, \n",
    "        tokenizer=tokenize_and_stem_single, \n",
    "    )\n",
    "    \n",
    "    #fit the vectorizer to data\n",
    "    matrix = tfidf_vectorizer.fit_transform(topics) \n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    return matrix, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa8dff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Topics will be list of articles that each article has a lot of sentences\n",
    "def tf_idf2(topics = []):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=0.95, \n",
    "        #max_features=200000, \n",
    "        min_df=0.05, \n",
    "        stop_words='english', \n",
    "        #use_idf=True, \n",
    "        tokenizer=tokenize_and_stem, \n",
    "    )\n",
    "    \n",
    "    #fit the vectorizer to data\n",
    "    matrix = tfidf_vectorizer.fit_transform(topics) \n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    return matrix, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c9b8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED ---------------------------------\n",
    "def count_vectorizer(topics = []):\n",
    "    cv = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2, \n",
    "        stop_words='english',\n",
    "        tokenizer=tokenize_and_stem_single,\n",
    "    )\n",
    "    dtm = cv.fit_transform(topics)\n",
    "    terms = cv.get_feature_names()\n",
    "    return matrix, terms\n",
    "# NOT USED ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4b8eea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_model(topics, matrix, terms, num_clusters=3, num_words_of_name=5):\n",
    "    \n",
    "    model = LatentDirichletAllocation(n_components=num_clusters, random_state=42)\n",
    "    model.fit(matrix)\n",
    "    \n",
    "    topic_results = model.transform(matrix)\n",
    "    \n",
    "    #final clusters\n",
    "    clusters = topic_results.argmax(axis=1)\n",
    "    topic_data = {'topic': topics, 'cluster': clusters }\n",
    "    frame = pd.DataFrame(topic_data, columns = ['cluster'])\n",
    "    \n",
    "    # Sorted Clusters (bigger to smaller) by number of docs per cluster \n",
    "    categories = frame['cluster'].value_counts().keys()\n",
    "\n",
    "    # printing top names for topic\n",
    "    names=[]\n",
    "    for index in categories:\n",
    "        name=\"\"\n",
    "        topic = model.components_[index]\n",
    "        num_words_of_name = min(len(topic), num_words_of_name)\n",
    "        for i in topic.argsort()[-num_words_of_name:]:\n",
    "            name += (terms[i]+\" \")\n",
    "        names.append(name)\n",
    "    \n",
    "    return names, frame, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6bd16ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_model(topics, matrix, terms, num_clusters=3, num_words_of_name=5):\n",
    "    \n",
    "    model = NMF(n_components = num_clusters, random_state=42)\n",
    "    model.fit(matrix)\n",
    "    \n",
    "    topic_results = model.transform(matrix)\n",
    "    \n",
    "    #final clusters\n",
    "    clusters = topic_results.argmax(axis=1)\n",
    "    topic_data = {'topic': topics, 'cluster': clusters }\n",
    "    frame = pd.DataFrame(topic_data, columns = ['cluster'])\n",
    "    \n",
    "    # Sorted Clusters (bigger to smaller) by number of docs per cluster \n",
    "    categories = frame['cluster'].value_counts().keys()\n",
    "\n",
    "    # printing top names for topic\n",
    "    names=[]\n",
    "    for index in categories:\n",
    "        name=\"\"\n",
    "        topic = model.components_[index]\n",
    "        num_words_of_name = min(len(topic), num_words_of_name)\n",
    "        for i in topic.argsort()[-num_words_of_name:]:\n",
    "            name += (terms[i]+\" \")\n",
    "        names.append(name)\n",
    "    \n",
    "    return names, frame, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f27f7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSA_model(topics, matrix, terms, num_clusters=3, num_words_of_name=5):\n",
    "    \n",
    "    model = TruncatedSVD(n_components = num_clusters)\n",
    "    model.fit(matrix)\n",
    "    \n",
    "    topic_results = model.transform(matrix)\n",
    "    \n",
    "    #final clusters\n",
    "    clusters = topic_results.argmax(axis=1)\n",
    "    topic_data = {'topic': topics, 'cluster': clusters }\n",
    "    frame = pd.DataFrame(topic_data, columns = ['cluster'])\n",
    "    \n",
    "    # Sorted Clusters (bigger to smaller) by number of docs per cluster \n",
    "    categories = frame['cluster'].value_counts().keys()\n",
    "\n",
    "    # printing top names for topic\n",
    "    names=[]\n",
    "    for index in categories:\n",
    "        name=\"\"\n",
    "        topic = model.components_[index]\n",
    "        num_words_of_name = min(len(topic), num_words_of_name)\n",
    "        for i in topic.argsort()[-num_words_of_name:]:\n",
    "            name += (terms[i]+\" \")\n",
    "        names.append(name)\n",
    "    \n",
    "    return names, frame, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6efa69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmean_model(topics, matrix, terms, num_clusters=3, num_words_of_name=5):\n",
    "    #Running clustering algorithm\n",
    "    model = KMeans(n_clusters=num_clusters)\n",
    "    model.fit(matrix)\n",
    "    \n",
    "    #final clusters -> Ex.[1, 1, 0, 0, 0, 2, 1, 2, 0]\n",
    "    clusters = model.labels_.tolist()\n",
    "    \n",
    "    topic_data = {'topic': topics, 'cluster': clusters }\n",
    "    frame = pd.DataFrame(topic_data, columns = ['cluster'])\n",
    "\n",
    "    # Sorted Clusters (bigger to smaller) by number of docs per cluster \n",
    "    categories = frame['cluster'].value_counts().keys()   # => [2, 0, 1]    \n",
    "        \n",
    "    #sort cluster centers by proximity to centroid\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    \n",
    "    # printing top names for topic\n",
    "    names=[]\n",
    "    for i in range(len(categories)):#num-Of-Clusters\n",
    "        name=\"\"\n",
    "        num_words_of_name = min( len(order_centroids[categories[i]]), num_words_of_name)\n",
    "        for index in order_centroids[categories[i], :num_words_of_name]:\n",
    "            name+= (terms[index] + \" \")\n",
    "        names.append(name)\n",
    "        \n",
    "    return names, frame, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "648fd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_kmeans_model(topics, matrix, terms, num_clusters=3, num_words_of_name=5):\n",
    "    #Running clustering algorithm\n",
    "    model = MiniBatchKMeans(n_clusters=num_clusters)\n",
    "    model.fit(matrix)\n",
    "    \n",
    "    #final clusters -> Ex.[1, 1, 0, 0, 0, 2, 1, 2, 0]\n",
    "    clusters = model.labels_.tolist()\n",
    "     \n",
    "    topic_data = {'topic': topics, 'cluster': clusters }\n",
    "    frame = pd.DataFrame(topic_data, columns = ['cluster'])\n",
    "    \n",
    "    # Sorted Clusters (bigger to smaller) by number of docs per cluster \n",
    "    categories = frame['cluster'].value_counts().keys()    \n",
    "    \n",
    "    #sort cluster centers by proximity to centroid\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    # printing top names for topic\n",
    "    names=[]\n",
    "    for i in range(len(categories)):#num-Of-Clusters\n",
    "        name=\"\"\n",
    "        num_words_of_name = min(len(order_centroids[categories[i]]), num_words_of_name)\n",
    "        for index in order_centroids[categories[i], :num_words_of_name]:\n",
    "            name+= (terms[index] + \" \")\n",
    "        names.append(name)\n",
    "        \n",
    "    return names, frame, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a95aab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_algorithm():\n",
    "    clear_listbox() # clear list box from any data\n",
    "    \n",
    "    selected_model = dropdown_model.get() # read the name of algorithm that will use\n",
    "    route = dropdown_choose.get() # read the route which will use (Individual / On Previous Data)\n",
    "    \n",
    "    numOfClusters = 3\n",
    "    numOfWords = 5\n",
    "    \n",
    "    if(len(entry_num_clusters.get()) > 0): #check if user input number of clusters\n",
    "        numOfClusters = int(entry_num_clusters.get())\n",
    "        \n",
    "    if(len(entry_num_words.get()) > 0): #check if user input number of words in each Topic Name\n",
    "        numOfWords = int(entry_num_words.get())\n",
    "    \n",
    "    article = text_area.get(\"1.0\", tk.END)  # Retrieve all text from the beginning to the end from Text Area\n",
    "    \n",
    "    \n",
    "    # check which route will be use to choose the right tf-idf function \n",
    "    #(Individual -> tf-idf), (On Previous Data -> tf-idf2),\n",
    "    topics =[]\n",
    "    if(route == \"Individual\"):\n",
    "        topics = sentence_seperator(article)\n",
    "        matrix, terms = tf_idf(topics)\n",
    "    else:\n",
    "        mini = 0\n",
    "        maxi = 20\n",
    "        if(len(entry_min_articles.get()) > 0): #check if user input minimum range if data\n",
    "            mini = int(entry_min_articles.get())\n",
    "        \n",
    "        if(len(entry_max_articles.get()) > 0): #check if user input maximum range if data\n",
    "            maxi = int(entry_max_articles.get())\n",
    "        \n",
    "        topics = Data[mini:maxi]\n",
    "        matrix, terms = tf_idf2(topics)\n",
    "    \n",
    "\n",
    "\n",
    "    # check which algorithm will used and call the function of it\n",
    "    # the parameters that send to model:\n",
    "    #     topics : all topics that algorithm should classified them to clustters \n",
    "    #     matrix : features data that algorithm will use\n",
    "    #     terms : features name\n",
    "    #     numOfClusters : number of clusters that will algorithm devide data on it\n",
    "    #     numOfWords : number of words in each topic name\n",
    "    names=[]\n",
    "    if(selected_model==\"KMeans\"):\n",
    "        names, frame, categories = kmean_model(topics, matrix, terms, numOfClusters, numOfWords)\n",
    "    elif(selected_model==\"MiniBatchKMeans\"):\n",
    "        names, frame, categories = mini_batch_kmeans_model(topics, matrix, terms, numOfClusters, numOfWords)\n",
    "    elif(selected_model==\"LDA\"):\n",
    "        names, frame, categories = LDA_model(topics, matrix, terms, numOfClusters, numOfWords)\n",
    "    elif(selected_model==\"LSA\"):\n",
    "        names, frame, categories = LSA_model(topics, matrix, terms, numOfClusters, numOfWords)\n",
    "    elif(selected_model==\"NMF\"):\n",
    "        names, frame, categories = NMF_model(topics, matrix, terms, numOfClusters, numOfWords)\n",
    "     \n",
    "    \n",
    "    \n",
    "    nameOfTopic = \"\"\n",
    "    if(route == \"Individual\"):\n",
    "        nameOfTopic = names[0] # choose the best topic name for this articale\n",
    "    else:             \n",
    "        nameOfTopic = getTopic(names, frame, categories, index_random) # get the topic name of this artical\n",
    "    \n",
    "\n",
    "    # array of persentages of articals in each cluster \n",
    "    persentages = ((frame['cluster'].value_counts().values/len(topics))*100).round(1)\n",
    "    \n",
    "    \n",
    "    # concatenate each cluster name with it persantage and add thim to list box\n",
    "    for i in range(numOfClusters):\n",
    "        per=0\n",
    "        if(i<len(persentages)): per = persentages[i]\n",
    "            \n",
    "        name=\"!!\"\n",
    "        if(i<len(names)): name = names[i]\n",
    "        \n",
    "        item = name + \"  ==>  \" + str(per) +\"%\"\n",
    "        add_to_list(item)\n",
    "        \n",
    "    \n",
    "    # add final altical topic name to list box\n",
    "    add_to_list(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "    if(route == \"Individual\"):\n",
    "        add_to_list(\"The Best Topic Name of This Article:\")\n",
    "    else:\n",
    "        add_to_list(\"The Topic Name of This Article:\")\n",
    "    \n",
    "    add_to_list(nameOfTopic)\n",
    "    \n",
    "    \n",
    "    # draw box blot for each article and colored it with it cluster color\n",
    "    graph_draw(matrix, names, categories, persentages, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5b273b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(names, frame, categories, index_random):\n",
    "    cluster = frame[\"cluster\"][index_random] # get cluster of this articale with index = index_random in list of Data\n",
    "    topic_name = \"\"\n",
    "    for i in range(len(categories)):\n",
    "        if(categories[i] == cluster): \n",
    "            topic_name = names[i] #get cluster name of index of categories list \n",
    "    \n",
    "    return topic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6cfe20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_all():\n",
    "    clear_entries()\n",
    "    clear_text_area()\n",
    "    clear_listbox()\n",
    "\n",
    "    \n",
    "def clear_text_area():\n",
    "    text_area.delete(\"1.0\", tk.END)\n",
    "    \n",
    "def clear_listbox():\n",
    "    listbox.delete(0, tk.END)\n",
    "    \n",
    "def clear_entries():\n",
    "    entry_num_clusters.delete(0, tk.END)\n",
    "    entry_num_words.delete(0, tk.END)\n",
    "    entry_min_articles.delete(0, tk.END)\n",
    "    entry_max_articles.delete(0, tk.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03689afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_list(topic_name):\n",
    "    listbox.insert(tk.END, topic_name)\n",
    "    \n",
    "\n",
    "index_random = 0\n",
    "mini = 0\n",
    "maxi = 20\n",
    "def generate_article():\n",
    "    clear_text_area()\n",
    "    \n",
    "    if(len(entry_min_articles.get()) > 0): #check if user input minimum range if data\n",
    "        mini = int(entry_min_articles.get())\n",
    "        \n",
    "    if(len(entry_max_articles.get()) > 0): #check if user input maximum range if data\n",
    "        maxi = int(entry_max_articles.get())\n",
    "        \n",
    "    #Generate Randmon index to select random topic\n",
    "    route = dropdown_choose.get()\n",
    "    if(route == \"Individual\"):\n",
    "        index_random = random.randint(0, len(Data))\n",
    "    else:\n",
    "        index_random = random.randint(mini, maxi)\n",
    "    article = Data[index_random]\n",
    "    \n",
    "    text_area.insert(tk.END, article) # Set text in the text area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "121f1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up colors hash examples for a lot of clusters using a dict\n",
    "cluster_colors = {\n",
    "    0:  '#1b9e77', \n",
    "    1:  '#d95f02', \n",
    "    2:  '#7570b3', \n",
    "    3:  '#e7298a', \n",
    "    4:  '#66a61e',\n",
    "    5:  '#D2691E',\n",
    "    6:  '#3333ff',\n",
    "    7:  '#cc00cc',\n",
    "    8:  '#ff4d94',\n",
    "    9:  '#ff3300',\n",
    "    10: '#e6e600',\n",
    "    11: '#73e600',\n",
    "    12: '#006600',\n",
    "    13: '#66ccff',\n",
    "    14: '#336699',\n",
    "    15: '#4d0099'\n",
    "}\n",
    "\n",
    "\n",
    "def graph_draw(matrix, names, categories, persentages, frame):\n",
    "    \n",
    "    new_names=[]\n",
    "    for i in range(len(names)):\n",
    "        s = names[i] + \" ==> \" + str(persentages[i]) +\"%\"\n",
    "        new_names.append(s)\n",
    "        \n",
    "    names = new_names\n",
    "    \n",
    "    #set up cluster names using a dict\n",
    "    cluster_names = dict(zip(categories, names))\n",
    "    \n",
    "    #Similarity\n",
    "    similarity_distance = 1 - cosine_similarity(matrix)\n",
    "    \n",
    "    # Convert two components as we're plotting points in a two-dimensional plane\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "    pos = mds.fit_transform(similarity_distance)  # shape (n_components, n_samples)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "    size = max(len(xs), len(ys))\n",
    "    xs = xs[:size]\n",
    "    ys = ys[:size]\n",
    "    \n",
    "    # Finally plot it\n",
    "    %matplotlib inline \n",
    "    \n",
    "    clusters = frame['cluster']\n",
    "\n",
    "    #Create data frame that has the result of the MDS and the cluster \n",
    "    df = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) \n",
    "    groups = df.groupby('label')\n",
    "\n",
    "    # Set up plot\n",
    "    fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x, group.y, marker='o', linestyle='', ms=20, \n",
    "                label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "        ax.set_aspect('auto')\n",
    "        ax.tick_params(axis= 'x', which='both', bottom='off', top='off', labelbottom='off')\n",
    "        ax.tick_params(axis= 'y', which='both', left='off', top='off', labelleft='off')\n",
    "    \n",
    "    \n",
    "    ax.legend(numpoints=1) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e0363d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------- GUI ----------\n",
    "\n",
    "x_dimensional = 1100\n",
    "y_dimensional = 650\n",
    "frm = tk.Tk()\n",
    "frm.title(\"Topic Modiling\")\n",
    "frm.geometry(str(x_dimensional)+\"x\"+str(y_dimensional))\n",
    "Label(frm, text=\"Topic Modiling\", font=(\"Kartika\", 40, \"underline\"), fg='#0000cc').place(x=350, y=10)\n",
    "title_color=\"#314f81\"\n",
    "\n",
    "y_direction = 120\n",
    "x_direction = 20\n",
    "\n",
    "\n",
    "s = \"Range Of Articles (0 - \" +str(len(Data)) + \") : \" \n",
    "Label(frm, text=s, font=(\"Arial\", 10)).place(x=20, y=90)\n",
    "entry_min_articles = tk.Entry(frm, width=10)\n",
    "entry_min_articles.pack()\n",
    "entry_min_articles.place(x=210, y=92)\n",
    "\n",
    "entry_max_articles = tk.Entry(frm, width=10)\n",
    "entry_max_articles.pack()\n",
    "entry_max_articles.place(x=280, y=92)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a variable to store the selected item\n",
    "dropdown_model = tk.StringVar(frm)\n",
    "# Create a list of options for the dropdown\n",
    "options = ['KMeans', 'MiniBatchKMeans', 'LDA', 'LSA', 'NMF']\n",
    "# Set the default value of the dropdown\n",
    "dropdown_model.set(options[0])\n",
    "# Create the dropdown widget\n",
    "dropdown_model_var = tk.OptionMenu(frm, dropdown_model, *options)\n",
    "dropdown_model_var.pack()\n",
    "dropdown_model_var.place(x=20, y=y_direction)\n",
    "\n",
    "\n",
    "\n",
    "# Create a variable to store the selected item\n",
    "dropdown_choose = tk.StringVar(frm)\n",
    "# Create a list of options for the dropdown\n",
    "options = ['Individual', 'On Previous Data']\n",
    "# Set the default value of the dropdown\n",
    "dropdown_choose.set(options[0])\n",
    "# Create the dropdown widget\n",
    "dropdown_choose_var = tk.OptionMenu(frm, dropdown_choose, *options)\n",
    "dropdown_choose_var.pack()\n",
    "dropdown_choose_var.place(x=150, y=y_direction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Label(frm, text=\"N. Clusters: \", font=(\"Arial\", 12)).place(x=290, y=y_direction+3)\n",
    "entry_num_clusters = tk.Entry(frm, width=10)\n",
    "entry_num_clusters.pack()\n",
    "entry_num_clusters.place(x=380, y=y_direction+4)\n",
    "\n",
    "\n",
    "\n",
    "Label(frm, text=\"N. Words: \", font=(\"Arial\", 12)).place(x=480, y=y_direction+3)\n",
    "entry_num_words = tk.Entry(frm, width=10)\n",
    "entry_num_words.pack()\n",
    "entry_num_words.place(x=560, y=y_direction+3)\n",
    "\n",
    "\n",
    "\n",
    "# Create a listbox to display the Topics Names\n",
    "listbox = tk.Listbox(frm, width=68, height=28)\n",
    "listbox.pack()\n",
    "listbox.place(x=x_dimensional-450, y=y_direction)\n",
    "\n",
    "\n",
    "\n",
    "text_area = tk.Text(frm, width=75, height=25)\n",
    "text_area.pack()\n",
    "text_area.place(x=x_direction, y=170)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Button(\n",
    "    frm, \n",
    "    text=\"Apply\", \n",
    "    command=apply_algorithm, \n",
    "    font=(\"Arial\", 18), \n",
    "    width=20, \n",
    "    height=1, \n",
    "    fg=\"#b30000\"\n",
    ").place(x=710, y=y_dimensional-60)\n",
    "\n",
    "\n",
    "Button(\n",
    "    frm, \n",
    "    text=\"Generate Random Article\", \n",
    "    command=generate_article, \n",
    "    font=(\"Arial\", 13), \n",
    "    width=22, \n",
    "    height=1, \n",
    "    fg=\"#b30000\"\n",
    ").place(x=260, y=y_dimensional-50)\n",
    "\n",
    "Button(\n",
    "    frm, \n",
    "    text=\"Clear\", \n",
    "    command=clear_all, \n",
    "    font=(\"Arial\", 13), \n",
    "    width=8, \n",
    "    height=1, \n",
    "    fg=\"#b30000\"\n",
    ").place(x=180, y=y_dimensional-50)\n",
    "\n",
    "Button(\n",
    "    frm, \n",
    "    text=\"Exit\", \n",
    "    command=frm.destroy, \n",
    "    font=(\"Arial\", 13), \n",
    "    width=8, \n",
    "    height=1, \n",
    "    fg=\"#b30000\"\n",
    ").place(x=20, y=y_dimensional-50)\n",
    "\n",
    "\n",
    "frm.mainloop() # run form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfda9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
